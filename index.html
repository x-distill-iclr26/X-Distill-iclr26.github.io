<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>X-Distill: Project Page</title>
    <style>
        :root {
            --text-color: #333333;
            --title-color: #2c3e50;
            --link-color: #2980b9;
            --bg-color: #ffffff;
            --section-bg-color: #f8f9fa;
            --border-color: #dee2e6;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.7;
            color: var(--text-color);
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: var(--bg-color);
        }
        h1, h2, h3, h4 {
            color: var(--title-color);
            border-bottom: 1px solid var(--border-color);
            padding-bottom: 10px;
            font-weight: 600;
        }
        h1 { font-size: 2.4em; letter-spacing: -1px; }
        h2 { font-size: 1.8em; margin-top: 50px; }
        h3 { font-size: 1.5em; border-bottom: none; margin-top: 40px;}
        h4 { font-size: 1.2em; border-bottom: none; margin-top: 30px; color: #555; text-align: left; padding-left: 10px; border-left: 3px solid var(--link-color);}
        a {
            color: var(--link-color);
            text-decoration: none;
            font-weight: 500;
        }
        a:hover {
            text-decoration: underline;
        }
        .paper-title {
            text-align: center;
            margin-bottom: 20px;
        }
        .paper-title p {
            font-size: 1.1em;
            color: #6c757d;
        }
        .links {
            text-align: center;
            font-size: 1.2em;
            margin-bottom: 40px;
            padding: 15px;
            background-color: var(--section-bg-color);
            border-radius: 8px;
        }
        .links a {
            margin: 0 15px;
        }
        .figure-container, .video-container {
            margin-bottom: 30px;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 15px;
            background-color: #fff;
            box-shadow: 0 2px 8px rgba(0,0,0,0.07);
        }
        .figure-container img, video {
            width: 100%;
            border-radius: 5px;
            background-color: #000;
        }
        .teaser-img {
            width: 100%;
            margin: 20px 0 40px 0;
            border-radius: 8px;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        #abstract {
            background-color: var(--section-bg-color);
            padding: 25px;
            border-radius: 8px;
            border-left: 4px solid var(--link-color);
            margin-bottom: 50px;
        }
        .caption {
            font-size: 0.95em;
            color: #555;
            margin-top: 10px;
            padding: 0 5px;
            text-align: left;
        }
        .footer {
            text-align: center;
            margin-top: 50px;
            padding-top: 20px;
            font-size: 0.9em;
            color: #888;
            border-top: 1px solid var(--border-color);
        }
    </style>
</head>
<body>

    <div class="paper-title">
        <h1>X-Distill: Cross-Architecture Vision Distillation Enables Data-Efficient Visuomotor Learning</h1>
        <p>Anonymous ICLR 2026 Submission</p>
    </div>

    <div class="links">
        <a href="#abstract">[Abstract]</a>
        <a href="#real-world">[Real-World Experiments]</a>
        <a href="#analysis">[Qualitative Analysis]</a>
    </div>

    

    <h2 id="abstract">Abstract</h2>
    <p>
        Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly finetuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on 34 simulated benchmarks and 5 challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or finetuned DINOv2 encoders. Notably, X-Distill also surpasses stronger baselines that utilize privileged 3D observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.
    </p>

    <!-- 核心修改: 将 Task Setup 和 Videos 合并到一个 Section -->
    <h2 id="real-world">Real-World Experiments</h2>
    <img src="teaser_figure.png" alt="Teaser Figure" class="teaser-img">
    <h3>Task Setup & Generalization</h3>
    <div class="figure-container">
        <img src="realworld_config.png" alt="Real-world task configurations">
        <p class="caption"><b>Visualization of configurations for our real-world tasks.</b> The orange arrow provides a schematic representation of the gripper trajectory as derived from the data. The green regions represent the distribution of object/robot configurations seen during training demonstrations, while the red regions illustrate the novel configurations used for generalization testing.</p>
    </div>

    <h3>Task-by-Task Video Results</h3>
    
    <h4>Task 1: Move Cube</h4>
    <div class="video-container">
        <video controls loop muted playsinline poster="cube_thumbnail.jpg">
            <source src="videos/move_cube_comparison.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
        <p class="caption"><b>4-Way Comparison of Move Cube task</b>.</p>
    </div>

    <h4>Task 2: Move Brush</h4>
    <div class="video-container">
        <video controls loop muted playsinline poster="brush_thumbnail.jpg">
            <source src="videos/move_brush_comparison.mp4" type="video/mp4"></video>
        <p class="caption"><b>4-Way Comparison of Move Brush task</b>.</p>
    </div>
    
    <h4>Task 3: Writing "AGI"</h4>
    <div class="video-container">
        <video controls loop muted playsinline poster="writing_thumbnail.jpg">
            <source src="videos/writing_agi_comparison.mp4" type="video/mp4"></video>
        <p class="caption"><b>4-Way Comparison of Writing "AGI" task</b>.</p>
    </div>
    <div class="video-container">
        <video controls loop muted playsinline poster="perturb_thumbnail.jpg">
            <source src="videos/writing_perturb_comparison.mp4" type="video/mp4"></video>
        <p class="caption"><b>Robustness to Perturbation:</b> X-Distill (left) robustly adapts, while ResNet-scratch (right) fails.</p>
    </div>

    <h4>Task 4: Drawer Open</h4>
    <div class="video-container">
        <video controls loop muted playsinline poster="drawer_thumbnail.jpg">
            <source src="videos/drawer_open_comparison.mp4" type="video/mp4"></video>
        <p class="caption"><b>4-Way Comparison of Drawer Open task</b>.</p>
    </div>
    
    <h4>Task 5: Door Close</h4>
    <div class="video-container">
        <video controls loop muted playsinline poster="door_thumbnail.jpg">
            <source src="videos/door_close_comparison.mp4" type="video/mp4"></video>
        <p class="caption"><b>4-Way Comparison of Door Close task</b>.</p>
    </div>


    <h2 id="analysis">Qualitative Analysis</h2>
    
    <h3>Feature Space Separability (t-SNE)</h3>
    <div class="figure-container">
        <img src="tsne.png" alt="t-SNE visualization of learned feature spaces">
        <p class="caption"><b>t-SNE visualization of learned feature spaces on the ''Writing AGI'' task.</b>b> Our X-Distill encoder learns to form three distinct clusters corresponding to the task's semantic stages, quantitatively confirming a well-separated feature space with a high Silhouette Score of 0.472, which indicates a high degree of cluster cohesion and separation compared with the baselines. This semantic separability is crucial for the policy to accurately identify the current task stage, enabling precise long-horizon planning for the sequential writing task.</p>
    </div>

    <h3>Saliency Map Visualization</h3>
    <div class="figure-container">
        <img src="saliency_map.png" alt="Saliency map comparison">
        <p class="caption"><b>Saliency map comparison on the ''Writing AGI'' task.</b> We visualize the model's visual focus at the beginning of each writing stage. Our X-Distill encoder correctly shifts its attention from the gripper (before 'A'), to the letter 'A' (before 'G'), and finally to the letter 'G' (before 'I'). Baseline models exhibit diffuse or irrelevant attention.</p>
    </div>

    <div class="footer">
        <p>ICLR 2026 Anonymous Submission</p>
    </div>

</body>
</html>

